{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reproducing-results.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reproducing results\n",
        "\n",
        "This notebook will walk you through reproducing the main results from [our paper](https://aclanthology.org/2022.bionlp-1.2/)."
      ],
      "metadata": {
        "id": "tsitRCnCCPAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Install the prerequisites"
      ],
      "metadata": {
        "id": "hiQfyZ6U8Yj8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Dak0SD8SDD"
      },
      "outputs": [],
      "source": [
        "# The colab environment comes with py3.7, but several dependencies require py>=3.8 (like NumPy).\n",
        "# This can be removed if Colab ever updates python to >=3.8 in its environment.\n",
        "# For the solution, see: https://stackoverflow.com/q/60775160/6578628\n",
        "# For the issue tracking Colab's python update, see: https://github.com/googlecolab/colabtools/issues/1880\n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!pip install ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/JohnGiorgi/seq2rel.git\n",
        "!pip install git+https://github.com/JohnGiorgi/seq2rel-ds.git"
      ],
      "metadata": {
        "id": "IDXGOUdS8Y8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = \"datasets\"\n",
        "OUTPUT_DIR = \"output\""
      ],
      "metadata": {
        "id": "sQisbdQb-eHx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CDR\n",
        "\n",
        "We will use the CDR corpus to explain the details, and the rest of the datasets will be given without comment."
      ],
      "metadata": {
        "id": "vEc_aWhZCrs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-end\n",
        "\n",
        "To evaluate the end-to-end models, all you need to do is provide the `model_name`, which can be any of the pretrained models found [here](https://github.com/JohnGiorgi/seq2rel/releases/tag/pretrained-models)"
      ],
      "metadata": {
        "id": "DNRXoNd1Ot30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cdr\""
      ],
      "metadata": {
        "id": "VjFtkm-YDIHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the directories to save the datasets and model results.\n",
        "preprocessed_data_dir = os.path.join(DATA_DIR, model_name)\n",
        "output_dir = os.path.join(OUTPUT_DIR, model_name)\n",
        "\n",
        "# AllenNLP doesn't create this directory for us, so create it here.\n",
        "!mkdir -p \"$output_dir\"\n",
        "\n",
        "# Set the url of the pretrained model\n",
        "pretrained_model_url = f\"https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/{model_name}.tar.gz\""
      ],
      "metadata": {
        "id": "L9L80yRb8nL5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can use [seq2rel-ds](https://github.com/JohnGiorgi/seq2rel-ds) to download and preprocess the dataset"
      ],
      "metadata": {
        "id": "Xw06fWtvDi6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!seq2rel-ds cdr main \"$preprocessed_data_dir\" --combine-train-valid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v7IkGSk9v4S",
        "outputId": "263bd9b1-6d2b-48e7-9a7b-b4f6359a59ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "======================================= Preprocessing CDR =======================================\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Downloaded the corpus.\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Training and validation sets will be combined into one train set.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Preprocessed the data.\u001b[0m\n",
            "\u001b[38;5;2m‚úî Preprocessed data saved to /content/datasets/cdr.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can evaluate this model using the [`allennlp evaluate`](https://docs.allennlp.org/main/api/commands/evaluate/) command"
      ],
      "metadata": {
        "id": "GTLg7wWbDu98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!allennlp evaluate \"$pretrained_model_url\" \"$preprocessed_data_dir/test.tsv\" \\\n",
        "    --output-file \"$output_dir/test_metrics.jsonl\" \\\n",
        "    --cuda-device 0 \\\n",
        "    --predictions-output-file \"$output_dir/test_predictions.jsonl\" \\\n",
        "    --include-package \"seq2rel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcyiabxz8eS6",
        "outputId": "774d4e62-e0f2-4d01-84df-307002d5def8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-04-15 20:51:41,976 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2022-04-15 20:51:43,635 - INFO - cached_path - cache of https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/cdr.tar.gz is up-to-date\n",
            "2022-04-15 20:51:43,635 - INFO - allennlp.models.archival - loading archive file https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/cdr.tar.gz from cache at /root/.allennlp/cache/da436b73452adc6becdb387839c37b12a8fbf93f16990fd6a63accdc56cc39c1.b998c18b6f8de86129a44212ca3cf410f9b51dafcb920e3e7211b62c89d602ff\n",
            "2022-04-15 20:51:43,636 - INFO - allennlp.models.archival - extracting archive file /root/.allennlp/cache/da436b73452adc6becdb387839c37b12a8fbf93f16990fd6a63accdc56cc39c1.b998c18b6f8de86129a44212ca3cf410f9b51dafcb920e3e7211b62c89d602ff to temp dir /tmp/tmpit3od2c5\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2022-04-15 20:52:02,758 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpit3od2c5/vocabulary.\n",
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2022-04-15 20:52:07,987 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2022-04-15 20:52:08,315 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpit3od2c5\n",
            "2022-04-15 20:52:08,379 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-04-15 20:52:08,381 - INFO - allennlp.commands.evaluate - Reading evaluation data from test\n",
            "loading instances: 0it [00:00, ?it/s]2022-04-15 20:52:08,383 - INFO - seq2rel.dataset_reader - Reading instances from lines in file at: datasets/cdr/test.tsv\n",
            "loading instances: 500it [00:03, 139.50it/s]\n",
            "2022-04-15 20:52:12,191 - INFO - allennlp.evaluation.evaluator - Iterating over dataset\n",
            "precision: 0.43, recall: 0.37, fscore: 0.40, loss: 110.81 ||: : 4it [04:30, 67.62s/it]\n",
            "2022-04-15 20:56:42,669 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"precision\": 0.4346405267715454,\n",
            "  \"recall\": 0.3746478855609894,\n",
            "  \"fscore\": 0.4024205803871155,\n",
            "  \"loss\": 110.80867385864258\n",
            "}\n",
            "2022-04-15 20:56:42,669 - INFO - allennlp.commands.evaluate - Finished evaluating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity hinting\n",
        "\n",
        "Evaluating the models using entity hinting works similarly, just prepend `_hints` to `model_name`"
      ],
      "metadata": {
        "id": "jKn2gNwfD82Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cdr_hints\""
      ],
      "metadata": {
        "id": "ypbXcHPEEK6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data_dir = os.path.join(DATA_DIR, model_name)\n",
        "output_dir = os.path.join(OUTPUT_DIR, model_name)\n",
        "!mkdir -p \"$output_dir\"\n",
        "pretrained_model_url = f\"https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/{model_name}.tar.gz\""
      ],
      "metadata": {
        "id": "AuIZII__EZY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and add the argument `--entity-hinting \"gold\"` to the call to `seq2rel-ds`"
      ],
      "metadata": {
        "id": "yBTW8oz2EP05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!seq2rel-ds cdr main \"$preprocessed_data_dir\" --combine-train-valid --entity-hinting \"gold\""
      ],
      "metadata": {
        "id": "xQrLNT9IEOFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The call to `allennlp evaluate` is unchanged"
      ],
      "metadata": {
        "id": "olOVLKFIEaJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes ~5min.\n",
        "!allennlp evaluate \"$pretrained_model_url\" \"$preprocessed_data_dir/test.tsv\" \\\n",
        "    --output-file \"$output_dir/test_metrics.jsonl\" \\\n",
        "    --cuda-device 0 \\\n",
        "    --predictions-output-file \"$output_dir/test_predictions.jsonl\" \\\n",
        "    --include-package \"seq2rel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odop32iJ85lj",
        "outputId": "e3fd49c6-4f4b-4282-fd24-df3381ea2538"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "======================================= Preprocessing CDR =======================================\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Downloaded the corpus.\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Entity hints will be inserted into the source text using the gold\n",
            "annotations.\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Training and validation sets will be combined into one train set.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Preprocessed the data.\u001b[0m\n",
            "\u001b[38;5;2m‚úî Preprocessed data saved to /content/datasets/cdr_hints.\u001b[0m\n",
            "2022-04-15 21:01:35,039 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2022-04-15 21:01:37,202 - INFO - cached_path - cache of https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/cdr_hints.tar.gz is up-to-date\n",
            "2022-04-15 21:01:37,202 - INFO - allennlp.models.archival - loading archive file https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/cdr_hints.tar.gz from cache at /root/.allennlp/cache/5d845bebc5887213bab7c90a311e51d6dff9a03fb60648a6498d58be8397166c.82548b1687f75978154d471c6ead95e2dd4d865a01baaba9fa7873d62232ffbe\n",
            "2022-04-15 21:01:37,203 - INFO - allennlp.models.archival - extracting archive file /root/.allennlp/cache/5d845bebc5887213bab7c90a311e51d6dff9a03fb60648a6498d58be8397166c.82548b1687f75978154d471c6ead95e2dd4d865a01baaba9fa7873d62232ffbe to temp dir /tmp/tmpiap2i9g3\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2022-04-15 21:01:56,540 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpiap2i9g3/vocabulary.\n",
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2022-04-15 21:02:01,937 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2022-04-15 21:02:02,253 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpiap2i9g3\n",
            "2022-04-15 21:02:02,317 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-04-15 21:02:02,321 - INFO - allennlp.commands.evaluate - Reading evaluation data from test\n",
            "loading instances: 0it [00:00, ?it/s]2022-04-15 21:02:02,322 - INFO - seq2rel.dataset_reader - Reading instances from lines in file at: datasets/cdr_hints/test.tsv\n",
            "loading instances: 500it [00:05, 84.53it/s]\n",
            "2022-04-15 21:02:08,463 - INFO - allennlp.evaluation.evaluator - Iterating over dataset\n",
            "precision: 0.68, recall: 0.66, fscore: 0.67, loss: 45.11 ||: : 4it [04:54, 73.60s/it]\n",
            "2022-04-15 21:07:02,857 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"precision\": 0.6824781894683838,\n",
            "  \"recall\": 0.6619718074798584,\n",
            "  \"fscore\": 0.6720686554908752,\n",
            "  \"loss\": 45.10655879974365\n",
            "}\n",
            "2022-04-15 21:07:02,857 - INFO - allennlp.commands.evaluate - Finished evaluating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GDA"
      ],
      "metadata": {
        "id": "pCbLka3QFRMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-end"
      ],
      "metadata": {
        "id": "dB2v9amHOxeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gda\"\n",
        "\n",
        "preprocessed_data_dir = os.path.join(DATA_DIR, model_name)\n",
        "output_dir = os.path.join(OUTPUT_DIR, model_name)\n",
        "!mkdir -p \"$output_dir\"\n",
        "pretrained_model_url = f\"https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/{model_name}.tar.gz\"\n",
        "\n",
        "!seq2rel-ds gda main \"$preprocessed_data_dir\"\n",
        "\n",
        "# Takes ~10min.\n",
        "!allennlp evaluate \"$pretrained_model_url\" \"$preprocessed_data_dir/test.tsv\" \\\n",
        "    --output-file \"$output_dir/test_metrics.jsonl\" \\\n",
        "    --cuda-device 0 \\\n",
        "    --predictions-output-file \"$output_dir/test_predictions.jsonl\" \\\n",
        "    --include-package \"seq2rel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjZX5-jA-a2p",
        "outputId": "a1553aaa-abee-4ff1-a2d8-2aecc9650452"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "======================================= Preprocessing GDA =======================================\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Downloaded the corpus.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Preprocessed the training data.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Preprocessed the test data.\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Holding out 20.00% of the training data as a validation set.\u001b[0m\n",
            "\u001b[38;5;2m‚úî Preprocessed data saved to /content/datasets/gda.\u001b[0m\n",
            "2022-04-15 21:15:53,401 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2022-04-15 21:15:55,568 - INFO - cached_path - cache of https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/gda.tar.gz is up-to-date\n",
            "2022-04-15 21:15:55,568 - INFO - allennlp.models.archival - loading archive file https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/gda.tar.gz from cache at /root/.allennlp/cache/473102e8cdb77dbc7cc8b70355bce7f765767b987cebe6b64028772bfd438f59.2c835eb34375fc9c9206dcfe8fa5ad0c17af767af48e3a2d332667b8d785b59b\n",
            "2022-04-15 21:15:55,569 - INFO - allennlp.models.archival - extracting archive file /root/.allennlp/cache/473102e8cdb77dbc7cc8b70355bce7f765767b987cebe6b64028772bfd438f59.2c835eb34375fc9c9206dcfe8fa5ad0c17af767af48e3a2d332667b8d785b59b to temp dir /tmp/tmp9slk1vg2\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2022-04-15 21:16:14,492 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmp9slk1vg2/vocabulary.\n",
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2022-04-15 21:16:19,394 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2022-04-15 21:16:19,684 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp9slk1vg2\n",
            "2022-04-15 21:16:19,739 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-04-15 21:16:19,742 - INFO - allennlp.commands.evaluate - Reading evaluation data from test\n",
            "loading instances: 0it [00:00, ?it/s]2022-04-15 21:16:19,744 - INFO - seq2rel.dataset_reader - Reading instances from lines in file at: datasets/gda/test.tsv\n",
            "loading instances: 1000it [00:08, 122.85it/s]\n",
            "2022-04-15 21:16:28,313 - INFO - allennlp.evaluation.evaluator - Iterating over dataset\n",
            "precision: 0.55, recall: 0.55, fscore: 0.55, loss: 30.62 ||: : 8it [09:17, 69.72s/it]\n",
            "2022-04-15 21:25:46,076 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"precision\": 0.5497347712516785,\n",
            "  \"recall\": 0.5537742376327515,\n",
            "  \"fscore\": 0.5517471432685852,\n",
            "  \"loss\": 30.621075868606567\n",
            "}\n",
            "2022-04-15 21:25:46,076 - INFO - allennlp.commands.evaluate - Finished evaluating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity hinting"
      ],
      "metadata": {
        "id": "ZrLIJHybFVMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gda_hints\"\n",
        "\n",
        "preprocessed_data_dir = os.path.join(DATA_DIR, model_name)\n",
        "output_dir = os.path.join(OUTPUT_DIR, model_name)\n",
        "\n",
        "!mkdir -p \"$output_dir\"\n",
        "pretrained_model_url = f\"https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/{model_name}.tar.gz\"\n",
        "\n",
        "!seq2rel-ds gda main \"$preprocessed_data_dir\" --entity-hinting \"gold\"\n",
        "\n",
        "# Takes ~10min.\n",
        "!allennlp evaluate \"$pretrained_model_url\" \"$preprocessed_data_dir/test.tsv\" \\\n",
        "    --output-file \"$output_dir/test_metrics.jsonl\" \\\n",
        "    --cuda-device 0 \\\n",
        "    --predictions-output-file \"$output_dir/test_predictions.jsonl\" \\\n",
        "    --include-package \"seq2rel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3gV3tQCFLSn",
        "outputId": "5cd905f2-d737-4e43-a557-bb1501512316"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "======================================= Preprocessing GDA =======================================\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Downloaded the corpus.\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Entity hints will be inserted into the source text using the gold\n",
            "annotations.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Preprocessed the training data.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Preprocessed the test data.\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Holding out 20.00% of the training data as a validation set.\u001b[0m\n",
            "\u001b[38;5;2m‚úî Preprocessed data saved to /content/datasets/gda_hints.\u001b[0m\n",
            "2022-04-15 21:29:32,675 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2022-04-15 21:29:34,629 - INFO - cached_path - https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/gda_hints.tar.gz not found in cache, downloading to /root/.allennlp/cache/85c523a0511e0717bf7736a8ca65fcf60a0b17a75370d2d03a4f2f4510a547ad.c3821cfed4a85d8d0ae199653b4c856e7506ab4786d45c182e0c3f47110f19ba\n",
            "downloading: 100%|##########| 420M/420M [00:47<00:00, 9.28MiB/s]\n",
            "2022-04-15 21:30:22,220 - INFO - allennlp.models.archival - loading archive file https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/gda_hints.tar.gz from cache at /root/.allennlp/cache/85c523a0511e0717bf7736a8ca65fcf60a0b17a75370d2d03a4f2f4510a547ad.c3821cfed4a85d8d0ae199653b4c856e7506ab4786d45c182e0c3f47110f19ba\n",
            "2022-04-15 21:30:22,221 - INFO - allennlp.models.archival - extracting archive file /root/.allennlp/cache/85c523a0511e0717bf7736a8ca65fcf60a0b17a75370d2d03a4f2f4510a547ad.c3821cfed4a85d8d0ae199653b4c856e7506ab4786d45c182e0c3f47110f19ba to temp dir /tmp/tmpdgavk2ql\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2022-04-15 21:30:41,324 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpdgavk2ql/vocabulary.\n",
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2022-04-15 21:30:46,907 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2022-04-15 21:30:47,202 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpdgavk2ql\n",
            "2022-04-15 21:30:47,266 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-04-15 21:30:47,269 - INFO - allennlp.commands.evaluate - Reading evaluation data from test\n",
            "loading instances: 0it [00:00, ?it/s]2022-04-15 21:30:47,270 - INFO - seq2rel.dataset_reader - Reading instances from lines in file at: datasets/gda_hints/test.tsv\n",
            "loading instances: 1000it [00:21, 47.48it/s]\n",
            "2022-04-15 21:31:08,779 - INFO - allennlp.evaluation.evaluator - Iterating over dataset\n",
            "precision: 0.84, recall: 0.85, fscore: 0.85, loss: 7.01 ||: : 8it [05:55, 44.46s/it]\n",
            "2022-04-15 21:37:04,477 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"precision\": 0.8440185189247131,\n",
            "  \"recall\": 0.8530393838882446,\n",
            "  \"fscore\": 0.848504900932312,\n",
            "  \"loss\": 7.013674020767212\n",
            "}\n",
            "2022-04-15 21:37:04,477 - INFO - allennlp.commands.evaluate - Finished evaluating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DGM\n",
        "\n",
        "The DGM corpus must be downloaded from [here](https://hanover.azurewebsites.net/downloads/naacl2019.aspx). The following expects that this corpus exists at `DATA_DIR/naacl2019`."
      ],
      "metadata": {
        "id": "7QGrpRWXFi7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-end"
      ],
      "metadata": {
        "id": "gaF200TKO0h4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"dgm\"\n",
        "\n",
        "# This expects that you have downloaded the DGM corpus and that it lives at DATA_DIR/naacl2019\n",
        "data_dir = os.path.join(DATA_DIR, \"naacl2019\")\n",
        "\n",
        "preprocessed_data_dir = os.path.join(DATA_DIR, model_name)\n",
        "output_dir = os.path.join(OUTPUT_DIR, model_name)\n",
        "!mkdir -p \"$output_dir\"\n",
        "pretrained_model_url = f\"https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/{model_name}.tar.gz\"\n",
        "\n",
        "!seq2rel-ds dgm main \"$data_dir\" \"$preprocessed_data_dir\"\n",
        "\n",
        "!allennlp evaluate \"$pretrained_model_url\" \"$preprocessed_data_dir/test.tsv\" \\\n",
        "    --output-file \"$output_dir/test_metrics.jsonl\" \\\n",
        "    --cuda-device 0 \\\n",
        "    --predictions-output-file \"$output_dir/test_predictions.jsonl\" \\\n",
        "    --include-package \"seq2rel\""
      ],
      "metadata": {
        "id": "5kSxBwokGEwQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity hinting"
      ],
      "metadata": {
        "id": "nzLUCwEnGza9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"dgm_hints\"\n",
        "\n",
        "# This expects that you have downloaded the DGM corpus and that it lives at DATA_DIR/naacl2019\n",
        "data_dir = os.path.join(DATA_DIR, \"naacl2019\")\n",
        "\n",
        "preprocessed_data_dir = os.path.join(DATA_DIR, model_name)\n",
        "output_dir = os.path.join(OUTPUT_DIR, model_name)\n",
        "!mkdir -p \"$output_dir\"\n",
        "pretrained_model_url = f\"https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/{model_name}.tar.gz\"\n",
        "\n",
        "!seq2rel-ds dgm main \"$data_dir\" \"$preprocessed_data_dir\" --entity-hinting \"gold\"\n",
        "\n",
        "!allennlp evaluate \"$pretrained_model_url\" \"$preprocessed_data_dir/test.tsv\" \\\n",
        "    --output-file \"$output_dir/test_metrics.jsonl\" \\\n",
        "    --cuda-device 0 \\\n",
        "    --predictions-output-file \"$output_dir/test_predictions.jsonl\" \\\n",
        "    --include-package \"seq2rel\""
      ],
      "metadata": {
        "id": "tC1RzRWIGv_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DocRED\n",
        "\n",
        "DocRED is only evaluated in the end-to-end setting."
      ],
      "metadata": {
        "id": "1BwtUyBgH_Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"docred\"\n",
        "\n",
        "preprocessed_data_dir = os.path.join(DATA_DIR, model_name)\n",
        "output_dir = os.path.join(OUTPUT_DIR, model_name)\n",
        "!mkdir -p \"$output_dir\"\n",
        "pretrained_model_url = f\"https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/{model_name}.tar.gz\"\n",
        "\n",
        "!seq2rel-ds docred main \"$preprocessed_data_dir\"\n",
        "\n",
        "# Takes ~30min.\n",
        "!allennlp evaluate \"$pretrained_model_url\" \"$preprocessed_data_dir/test.tsv\" \\\n",
        "    --output-file \"$output_dir/test_metrics.jsonl\" \\\n",
        "    --cuda-device 0 \\\n",
        "    --predictions-output-file \"$output_dir/test_predictions.jsonl\" \\\n",
        "    --include-package \"seq2rel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vWq5lj-H7tJ",
        "outputId": "6ec0dfbd-4911-4e96-d174-e123e1c63685"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "====================================== Preprocessing DocRED ======================================\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Downloaded the corpus.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2m‚úî Preprocessed the data.\u001b[0m\n",
            "\u001b[38;5;2m‚úî Preprocessed data saved to /content/datasets/docred.\u001b[0m\n",
            "2022-04-15 21:48:17,659 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2022-04-15 21:48:19,999 - INFO - cached_path - cache of https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/docred.tar.gz is up-to-date\n",
            "2022-04-15 21:48:19,999 - INFO - allennlp.models.archival - loading archive file https://github.com/JohnGiorgi/seq2rel/releases/download/pretrained-models/docred.tar.gz from cache at /root/.allennlp/cache/fc93f4b028785d1b77ece2ed95d5a278dbd8f48bb77b9be3c0e0b0713a694fe5.36dd151a95000c7b53adee599c6002eb74b5393a52d4f9044eb78c8c82c186b5\n",
            "2022-04-15 21:48:20,000 - INFO - allennlp.models.archival - extracting archive file /root/.allennlp/cache/fc93f4b028785d1b77ece2ed95d5a278dbd8f48bb77b9be3c0e0b0713a694fe5.36dd151a95000c7b53adee599c6002eb74b5393a52d4f9044eb78c8c82c186b5 to temp dir /tmp/tmpvib47fmm\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2022-04-15 21:48:37,841 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpvib47fmm/vocabulary.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2022-04-15 21:48:43,055 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2022-04-15 21:48:43,389 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpvib47fmm\n",
            "2022-04-15 21:48:43,452 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-04-15 21:48:43,455 - INFO - allennlp.commands.evaluate - Reading evaluation data from test\n",
            "loading instances: 0it [00:00, ?it/s]2022-04-15 21:48:43,457 - INFO - seq2rel.dataset_reader - Reading instances from lines in file at: datasets/docred/test.tsv\n",
            "loading instances: 29it [00:00, 287.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
            "loading instances: 525it [00:02, 288.33it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
            "loading instances: 700it [00:02, 246.01it/s]\n",
            "2022-04-15 21:48:46,635 - INFO - allennlp.evaluation.evaluator - Iterating over dataset\n",
            "precision: 0.44, recall: 0.34, fscore: 0.38, loss: 181.73 ||: : 6it [30:53, 308.88s/it]\n",
            "2022-04-15 22:19:39,889 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"precision\": 0.44003579020500183,\n",
            "  \"recall\": 0.33791524171829224,\n",
            "  \"fscore\": 0.3822729289531708,\n",
            "  \"loss\": 181.73338317871094\n",
            "}\n",
            "2022-04-15 22:19:39,889 - INFO - allennlp.commands.evaluate - Finished evaluating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to reproduce the main results from our paper. Please see [our paper](https://aclanthology.org/2022.bionlp-1.2/) and [repo](https://github.com/JohnGiorgi/seq2rel) for more details, and don't hesitate to open an issue if you have any trouble!\n",
        "\n"
      ],
      "metadata": {
        "id": "JowaUd-cKjhL"
      }
    }
  ]
}