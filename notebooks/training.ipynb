{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training your own model\n",
        "\n",
        "This notebook will walk you through training your own model using [seq2rel](https://github.com/JohnGiorgi/seq2rel)."
      ],
      "metadata": {
        "id": "-eqmYR91ZdfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Install the prerequisites"
      ],
      "metadata": {
        "id": "2L85SXAYZbIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The colab environment comes with py3.7, but several dependencies require py>=3.8 (like NumPy).\n",
        "# This can be removed if Colab ever updates python to >=3.8 in its environment.\n",
        "# For the solution, see: https://stackoverflow.com/q/60775160/6578628\n",
        "# For the issue tracking Colab's python update, see: https://github.com/googlecolab/colabtools/issues/1880\n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!pip install ipykernel"
      ],
      "metadata": {
        "id": "xp9ADCWKvv3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/JohnGiorgi/seq2rel.git\n",
        "!pip install git+https://github.com/JohnGiorgi/seq2rel-ds.git"
      ],
      "metadata": {
        "id": "VDcn5A7nZhNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“– Preparing a dataset\n",
        "\n",
        "Datasets are tab-separated files, where each example is contained on its own line. The first column contains the text, and the second column contains the relations. Relations themselves must be serialized to strings.\n",
        "\n",
        "Take the following example, which expresses a _gene-disease association_ (`\"@GDA@\"`) between _ESR1_ (`\"@GENE@\"`) and _schizophrenia_ (`\"@DISEASE@`\")\n",
        "\n",
        "```\n",
        "Variants in the estrogen receptor alpha (ESR1) gene and its mRNA contribute to risk for schizophrenia. estrogen receptor alpha ; ESR1 @GENE@ schizophrenia @DISEASE@ @GDA@\n",
        "```\n",
        "\n",
        "For convenience, we provide a second package, [seq2rel-ds](https://github.com/JohnGiorgi/seq2rel-ds), which makes it easy to generate data in this format for various popular corpora. In this tutorial, we will preprocess and train on the [GDA corpus](https://www.researchgate.net/publication/332411712_RENET_A_Deep_Learning_Approach_for_Extracting_Gene-Disease_Associations_from_Literature).\n",
        "\n",
        "> See [our paper](https://aclanthology.org/2022.bionlp-1.2/) for more details on serializing relations."
      ],
      "metadata": {
        "id": "MIZ3fNI9ZnNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_datadir = \"gda\"\n",
        "\n",
        "# Here, we hold out only 1% of training data so that validation is quick.\n",
        "# In the paper, we hold out 20%, which is the default value for --valid-size.\n",
        "!seq2rel-ds gda main \"$preprocessed_datadir\" --valid-size 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKAmWzh3ZtEC",
        "outputId": "02205174-868f-45bc-b079-c205aa84c62b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "======================================= Preprocessing GDA =======================================\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2mâœ” Downloaded the corpus.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2mâœ” Preprocessed the training data.\u001b[0m\n",
            "\u001b[2K\u001b[38;5;2mâœ” Preprocessed the test data.\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Holding out 1.00% of the training data as a validation set.\u001b[0m\n",
            "\u001b[38;5;2mâœ” Preprocessed data saved to /content/gda.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets confirm that our dataset looks as expected."
      ],
      "metadata": {
        "id": "x3cTAurs1mGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"$preprocessed_datadir\"  # This directory should contain three files, train.tsv, valid.tsv, and test.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaAFO8Oo0tVC",
        "outputId": "757279e5-1e97-4af0-c12f-62136bba6534"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.tsv  train.tsv  valid.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l \"$preprocessed_datadir/train.tsv\"  # This file should contain 28899 lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmI25PM21xIT",
        "outputId": "49902f19-ccc8-4e80-c150-47010975eab9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28899 gda/train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 1 \"$preprocessed_datadir/train.tsv\"  # This should be a single tab-seperated example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx-GR_Fi16Oh",
        "outputId": "4a2a87d1-894a-4676-fc9f-9e196b3a0bbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The fractalkine receptor CX3CR1 is involved in liver fibrosis due to chronic hepatitis C infection. BACKGROUND/AIMS: The chemokine receptor CX3CR1 and its specific ligand fractalkine (CX3CL1) are known to modulate inflammatory and fibroproliferative diseases. Here we investigate the role of CX3CR1/fractalkine in HCV-induced liver fibrosis. METHODS: A genotype analysis of CX3CR1 variants was performed in 211 HCV-infected patients. Hepatic expression of CX3CR1 was studied in HCV-infected livers and isolated liver cell populations by RT-PCR and immunohistochemistry. The effects of fractalkine on mRNA expression of profibrogenic genes were determined in isolated hepatic stellate cells (HSC) and CX3CR1 genotypes were related to intrahepatic TIMP-1 mRNA levels. RESULTS: The intrahepatic mRNA expression of CX3CR1 correlates with the stage of HCV-induced liver fibrosis (P=0.03). The CX3CR1 coding variant V249I is associated with advanced liver fibrosis, independent of the T280M variant (P=0.009). CX3CR1 is present on primary HSC and fractalkine leads to a suppression of tissue inhibitor of metalloproteinase (TIMP)-1 mRNA in HSC (P=0.03). Furthermore, CX3CR1 genotypes are associated with TIMP-1 mRNA expression in HCV-infected liver (P=0.03). CONCLUSIONS: The results identify the fractalkine receptor CX3CR1 as susceptibility a gene for hepatic fibrosis in HCV infection. The modulation of TIMP-1 expression by fractalkine and CX3CR1 genotypes provides functional support for the observed genotype-phenotype association.\tcx3cr1 @GENE@ liver fibrosis @DISEASE@ @GDA@ cx3cr1 @GENE@ chronic hepatitis c infection @DISEASE@ @GDA@\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸƒ Training the model\n",
        "\n",
        "Once you have collected the dataset, you can initiate a training session with the [`allennlp train`](https://docs.allennlp.org/main/api/commands/train/) command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config used to train the model on the [GDA corpus](https://link.springer.com/chapter/10.1007/978-3-030-17083-7_17) used in [our paper](https://aclanthology.org/2022.bionlp-1.2/):"
      ],
      "metadata": {
        "id": "0bCHcrM8bDQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_filepath = \"gda.jsonnet\"\n",
        "!wget -nc https://raw.githubusercontent.com/JohnGiorgi/seq2rel/main/training_config/gda.jsonnet -O {config_filepath}\n",
        "with open(config_filepath, \"r\") as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyOsxOo5bD17",
        "outputId": "b418a94f-5020-4226-9b69-74e04474367f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-15 16:10:29--  https://raw.githubusercontent.com/JohnGiorgi/seq2rel/main/training_config/gda.jsonnet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7940 (7.8K) [text/plain]\n",
            "Saving to: â€˜gda.jsonnetâ€™\n",
            "\n",
            "gda.jsonnet         100%[===================>]   7.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-15 16:10:29 (83.1 MB/s) - â€˜gda.jsonnetâ€™ saved [7940/7940]\n",
            "\n",
            "// =================== Configurable Settings ======================\n",
            "\n",
            "// The pretrained model to use as encoder. This is a reasonable default for biomedical text.\n",
            "// Should be a registered name in the Transformers library (see https://huggingface.co/models) \n",
            "// OR a path on disk to a serialized transformer model.\n",
            "local model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\";\n",
            "\n",
            "// These are reasonable defaults.\n",
            "local max_length = 512;       // Max length of input text\n",
            "local max_steps = 96;         // Max number of decoding steps\n",
            "\n",
            "local num_epochs = 30;        // Number of training epochs\n",
            "local batch_size = 4;         // Per-GPU batch size\n",
            "local grad_acc_steps = 1;     // Number of training steps before backpropagating gradients\n",
            "local decoder_lr = 5e-4;      // Learning rate for decoder params\n",
            "\n",
            "local encoder_lr = 2e-5;      // Learning rate for encoder params\n",
            "local encoder_wd = 0.01;      // Weight decay for encoder params\n",
            "local reinit_layers = 1;      // Re-initializes the last N layers of the encoder\n",
            "local dropout = 0.10;         // Dropout applied to decoder inputs and cross-attention weights\n",
            "local weight_dropout = 0.50;  // Weight dropout applied to hidden-to-hidden decoder weights\n",
            "\n",
            "local beam_size = 4;          // Beam size to use during decoding (test time only)\n",
            "local length_penalty = 0.8;   // >1.0 favours longer decodings and <1.0 shorter (test time only)\n",
            "\n",
            "// Number of GPUs to use. 0 means CPU only, 1 means one GPU, etc.\n",
            "local num_gpus = 1;\n",
            "\n",
            "// Set to `true` to use automatic mixed precision.\n",
            "local use_amp = true;\n",
            "\n",
            "// ================================================================\n",
            "\n",
            "// Lists containing the special entity/relation tokens in your target vocabulary\n",
            "local ent_tokens = [\n",
            "    \"@GENE@\",\n",
            "    \"@DISEASE@\",\n",
            "];\n",
            "local rel_tokens = [\n",
            "    \"@GDA@\",\n",
            "];\n",
            "\n",
            "// These are provided as external variables\n",
            "local train_data_path = std.extVar(\"train_data_path\");\n",
            "local validation_data_path = std.extVar(\"valid_data_path\");\n",
            "local dataset_size = std.parseInt(std.extVar('dataset_size'));\n",
            "\n",
            "// Validation will begin at the end of this epoch.\n",
            "local validation_start = std.max(std.floor(num_epochs - 2), 0);\n",
            "// ...and continues for every validation_interval epochs after that\n",
            "local validation_interval = 1;\n",
            "\n",
            "// ------ !! You probably don't need to edit below here !! --------\n",
            "\n",
            "// Learning rate will be linearly increased for the first 10% of training steps.\n",
            "local warmup_steps = std.floor(dataset_size / batch_size * num_epochs * 0.10);\n",
            "\n",
            "// Assumes relation labels match the special relation tokens minus the \"@\" symbol\n",
            "local rel_labels = [std.stripChars(token, \"@\") for token in rel_tokens];\n",
            "// Special tokens used in the source and target strings\n",
            "local special_target_tokens = ent_tokens + rel_tokens + [\";\", \"@start@\", \"@end@\"];\n",
            "\n",
            "// Define source and target namespaces\n",
            "local source_namespace = \"source_tokens\";\n",
            "local target_namespace = \"target_tokens\";\n",
            "// Determines which namespace the bucket batch sampler will sort on\n",
            "local sorting_keys = [source_namespace];\n",
            "\n",
            "// Setup source tokenizer\n",
            "local source_tokenizer_kwargs = {\n",
            "    \"do_lower_case\": true\n",
            "};\n",
            "local SOURCE_TOKENIZER = {\n",
            "    \"type\": \"pretrained_transformer\",\n",
            "    \"model_name\": model_name,\n",
            "    \"max_length\": max_length,\n",
            "    \"add_special_tokens\": true,\n",
            "    \"tokenizer_kwargs\": source_tokenizer_kwargs\n",
            "};\n",
            "\n",
            "// Setup target tokenizer\n",
            "local target_tokenizer_kwargs = {\n",
            "    \"additional_special_tokens\": special_target_tokens,\n",
            "    \"do_lower_case\": true\n",
            "};\n",
            "local TARGET_TOKENIZER = {\n",
            "    \"type\": \"pretrained_transformer\",\n",
            "    \"model_name\": model_name,\n",
            "    \"add_special_tokens\": false,\n",
            "    \"tokenizer_kwargs\": target_tokenizer_kwargs\n",
            "};\n",
            "\n",
            "{\n",
            "    \"vocabulary\": {\n",
            "        // This is a hacky way to ensure the target vocab contains only the\n",
            "        // special tokens (i.e. the COPY token and anything in tokens_to_add)\n",
            "        \"max_vocab_size\": {\n",
            "            [target_namespace]: 1\n",
            "        },\n",
            "        \"tokens_to_add\" : {\n",
            "            [target_namespace]: special_target_tokens\n",
            "        },\n",
            "    },\n",
            "    \"train_data_path\": train_data_path,\n",
            "    \"validation_data_path\": validation_data_path,\n",
            "    \"dataset_reader\": {\n",
            "        \"type\": \"seq2rel\",\n",
            "        \"max_length\": max_length,\n",
            "        \"target_namespace\": target_namespace,\n",
            "        \"source_tokenizer\": SOURCE_TOKENIZER,\n",
            "        \"target_tokenizer\": TARGET_TOKENIZER,\n",
            "        \"source_token_indexers\": {\n",
            "            \"tokens\": {\n",
            "                \"type\": \"pretrained_transformer\",\n",
            "                \"model_name\": model_name,\n",
            "                \"tokenizer_kwargs\": source_tokenizer_kwargs,\n",
            "            }\n",
            "        },\n",
            "    },\n",
            "    \"model\": {\n",
            "        \"type\": \"copynet_seq2rel\",\n",
            "        \"source_embedder\": {\n",
            "            \"token_embedders\": {\n",
            "                \"tokens\": {\n",
            "                    \"type\": \"pretrained_transformer\",\n",
            "                    \"model_name\": model_name,\n",
            "                    \"tokenizer_kwargs\": source_tokenizer_kwargs,\n",
            "                    \"reinit_modules\": reinit_layers,\n",
            "                },\n",
            "            },\n",
            "        },\n",
            "        \"target_tokenizer\": TARGET_TOKENIZER,\n",
            "        \"dropout\": dropout,\n",
            "        \"weight_dropout\": weight_dropout,\n",
            "        \"sequence_based_metrics\": [\n",
            "            {\n",
            "                \"type\": \"f1_seq2rel\",\n",
            "                \"labels\": rel_labels,\n",
            "                \"average\": \"micro\",\n",
            "                \"remove_duplicate_ents\": true,\n",
            "            },\n",
            "        ],\n",
            "        \"attention\": {\n",
            "            \"type\": \"multihead_attention\",\n",
            "            \"num_heads\": 6,\n",
            "            \"dropout\": dropout,\n",
            "        },\n",
            "        \"target_embedding_dim\": 256,\n",
            "        \"beam_search\": {\n",
            "            \"max_steps\": max_steps,\n",
            "            \"beam_size\": beam_size,\n",
            "            \"final_sequence_scorer\": {\n",
            "                \"type\": \"length-normalized-sequence-log-prob\",\n",
            "                // Larger values favour longer decodings and vice versa\n",
            "                \"length_penalty\": length_penalty,\n",
            "            },\n",
            "        },\n",
            "    },\n",
            "    \"data_loader\": {\n",
            "        \"batch_sampler\": {\n",
            "            \"type\": \"bucket\",\n",
            "            \"batch_size\": batch_size,\n",
            "            \"sorting_keys\": sorting_keys,\n",
            "        },\n",
            "    },\n",
            "    \"validation_data_loader\": {\n",
            "        \"batch_sampler\": {\n",
            "            \"type\": \"bucket\",\n",
            "            // To speed up validation, we set the batch size to a multiple of\n",
            "            // the batch size used during training.\n",
            "            \"batch_size\": batch_size * 32,\n",
            "            \"sorting_keys\": sorting_keys,\n",
            "            \"padding_noise\": 0.0,\n",
            "        },\n",
            "    },\n",
            "        \"trainer\": {\n",
            "        \"num_epochs\": num_epochs,\n",
            "        \"validation_metric\": \"+fscore\",\n",
            "        \"num_gradient_accumulation_steps\": grad_acc_steps,\n",
            "        \"optimizer\": {\n",
            "            \"type\": \"huggingface_adamw\",\n",
            "            \"lr\": decoder_lr,\n",
            "            \"eps\": 1e-8,\n",
            "            \"weight_decay\": 0.0,\n",
            "            \"correct_bias\": true,\n",
            "            \"parameter_groups\": [\n",
            "                // All parameters of the transformer excluding biases and LayerNorm\n",
            "                // RegEx unit tests: https://regex101.com/r/e5MakA/1\n",
            "                [\n",
            "                    [\"transformer_model(?!.*(?:bias|LayerNorm|layer_norm))\"],\n",
            "                    {\"lr\": encoder_lr, \"weight_decay\": encoder_wd}\n",
            "                ],\n",
            "                // All parameters of of the transformer that include biases and LayerNorm\n",
            "                // RegEx unit tests: https://regex101.com/r/RWo1yv/1\n",
            "                [\n",
            "                    [\"transformer_model(?=.*(?:bias|LayerNorm|layer_norm))\"],\n",
            "                    {\"lr\": encoder_lr, \"weight_decay\": 0.0}\n",
            "                ],\n",
            "            ],  \n",
            "        },\n",
            "        \"learning_rate_scheduler\": {\n",
            "            \"type\": \"linear_with_warmup\",\n",
            "            \"warmup_steps\": warmup_steps\n",
            "        },\n",
            "        \"grad_norm\": 1.0,\n",
            "        \"use_amp\": use_amp,\n",
            "        \"callbacks\": [\n",
            "            {\n",
            "                \"type\": \"should_validate_callback\",\n",
            "                \"validation_start\": validation_start,\n",
            "                \"validation_interval\": validation_interval,\n",
            "            },\n",
            "\t    ],\n",
            "    },\n",
            "    [if num_gpus > 1 then \"distributed\"]: {\n",
            "        \"cuda_devices\": std.range(0, num_gpus - 1),\n",
            "    },\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only additional information we need to provide is the path to the train and validation sets and the size of the dataset, which is needed to set the learning rate warmup ratio. We can do this with the enviornment variables `train_data_path`, `valid_data_path` and `dataset_size`"
      ],
      "metadata": {
        "id": "hIyYHRoeylg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "train_data_path = Path(preprocessed_datadir) / \"train.tsv\"\n",
        "valid_data_path = Path(preprocessed_datadir) / \"valid.tsv\"\n",
        "# Get the number of examples in the train set\n",
        "dataset_size = len(Path(\"gda/train.tsv\").read_text().strip().split(\"\\n\"))"
      ],
      "metadata": {
        "id": "RhKQxbwd0Jg1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because training the model on the entire dataset takes ~12hrs, we will train on a fraction of the dataset. We can modify the parameters in the config file via the `--overrides` argument (but you can also modify them in your config file directly, if you prefer):\n",
        "\n",
        "> Note: This model will not converge and obtains an F1-score close to zero. To actually train a model to good performance in a reasonable amount of time, you will likely need more powerful hardware than Colab provides (we used a V100-32GB in our paper) and _at least_ a few hundred training examples."
      ],
      "metadata": {
        "id": "N5x7fPG7AQXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "overrides = {\n",
        "    # validate after the last epoch only\n",
        "    \"trainer.callbacks.0.validation_start\": 29,\n",
        "    # load only a fraction of the train set\n",
        "    \"dataset_reader.max_instances\": 16\n",
        "}\n",
        "# Necessary to pass to the allennlp train command without errors\n",
        "overrides = json.dumps(overrides).replace('\"', \"'\")"
      ],
      "metadata": {
        "id": "9Es0vqYv_YiH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This should train and evaluate in ~3min.\n",
        "!train_data_path=\"$train_data_path\" \\\n",
        "valid_data_path=\"$valid_data_path\" \\\n",
        "dataset_size=\"$dataset_size\" \\\n",
        "allennlp train \"$config_filepath\" \\\n",
        "    --serialization-dir \"output\" \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"seq2rel\" \\\n",
        "    -f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HvRA7WObdjq",
        "outputId": "f10d7f26-e5de-47ee-e6f3-7a31e47713b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-04-15 16:10:54,398 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2022-04-15 16:10:55,611 - INFO - allennlp.common.params - evaluation = None\n",
            "2022-04-15 16:10:55,611 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2022-04-15 16:10:55,611 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2022-04-15 16:10:55,611 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2022-04-15 16:10:55,612 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2022-04-15 16:10:55,612 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-04-15 16:10:55,612 - INFO - allennlp.common.params - type = default\n",
            "2022-04-15 16:10:55,613 - INFO - allennlp.common.params - dataset_reader.type = seq2rel\n",
            "2022-04-15 16:10:55,613 - INFO - allennlp.common.params - dataset_reader.max_instances = 16\n",
            "2022-04-15 16:10:55,613 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2022-04-15 16:10:55,613 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2022-04-15 16:10:55,613 - INFO - allennlp.common.params - dataset_reader.target_namespace = target_tokens\n",
            "2022-04-15 16:10:55,613 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.type = pretrained_transformer\n",
            "2022-04-15 16:10:55,614 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
            "2022-04-15 16:10:55,614 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.add_special_tokens = True\n",
            "2022-04-15 16:10:55,614 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.max_length = 512\n",
            "2022-04-15 16:10:55,614 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.tokenizer_kwargs.do_lower_case = True\n",
            "2022-04-15 16:10:55,614 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.verification_tokens = None\n",
            "2022-04-15 16:11:13,532 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = pretrained_transformer\n",
            "2022-04-15 16:11:13,532 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
            "2022-04-15 16:11:13,533 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.add_special_tokens = False\n",
            "2022-04-15 16:11:13,533 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.max_length = None\n",
            "2022-04-15 16:11:13,533 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.tokenizer_kwargs.additional_special_tokens = ['@GENE@', '@DISEASE@', '@GDA@', ';', '@start@', '@end@']\n",
            "2022-04-15 16:11:13,533 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.tokenizer_kwargs.do_lower_case = True\n",
            "2022-04-15 16:11:13,533 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.verification_tokens = None\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2022-04-15 16:11:31,444 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.type = pretrained_transformer\n",
            "2022-04-15 16:11:31,444 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.token_min_padding_length = 0\n",
            "2022-04-15 16:11:31,444 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
            "2022-04-15 16:11:31,444 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.namespace = tags\n",
            "2022-04-15 16:11:31,444 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.max_length = None\n",
            "2022-04-15 16:11:31,445 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.tokenizer_kwargs.do_lower_case = True\n",
            "2022-04-15 16:11:31,446 - INFO - allennlp.common.params - dataset_reader.max_length = 512\n",
            "2022-04-15 16:11:31,446 - INFO - allennlp.common.params - train_data_path = gda/train.tsv\n",
            "2022-04-15 16:11:31,446 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2022-04-15 16:11:31,447 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2022-04-15 16:11:31,447 - INFO - allennlp.common.params - validation_data_path = gda/valid.tsv\n",
            "2022-04-15 16:11:31,447 - INFO - allennlp.common.params - test_data_path = None\n",
            "2022-04-15 16:11:31,447 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2022-04-15 16:11:31,447 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2022-04-15 16:11:31,447 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2022-04-15 16:11:31,447 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 4\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['source_tokens']\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2022-04-15 16:11:31,448 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2022-04-15 16:11:31,449 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2022-04-15 16:11:31,449 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2022-04-15 16:11:31,449 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2022-04-15 16:11:31,449 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2022-04-15 16:11:31,449 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2022-04-15 16:11:31,449 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2022-04-15 16:11:31,449 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f683d5d7d60>\n",
            "loading instances: 0it [00:00, ?it/s]2022-04-15 16:11:31,450 - INFO - seq2rel.dataset_reader - Reading instances from lines in file at: gda/train.tsv\n",
            "loading instances: 16it [00:00, 198.69it/s]\n",
            "2022-04-15 16:11:31,530 - INFO - allennlp.common.params - validation_data_loader.type = multiprocess\n",
            "2022-04-15 16:11:31,531 - INFO - allennlp.common.params - validation_data_loader.batch_size = None\n",
            "2022-04-15 16:11:31,531 - INFO - allennlp.common.params - validation_data_loader.drop_last = False\n",
            "2022-04-15 16:11:31,531 - INFO - allennlp.common.params - validation_data_loader.shuffle = False\n",
            "2022-04-15 16:11:31,531 - INFO - allennlp.common.params - validation_data_loader.batch_sampler.type = bucket\n",
            "2022-04-15 16:11:31,531 - INFO - allennlp.common.params - validation_data_loader.batch_sampler.batch_size = 128\n",
            "2022-04-15 16:11:31,531 - INFO - allennlp.common.params - validation_data_loader.batch_sampler.sorting_keys = ['source_tokens']\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.batch_sampler.padding_noise = 0\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.batch_sampler.drop_last = False\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.batch_sampler.shuffle = True\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.batches_per_epoch = None\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.num_workers = 0\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.max_instances_in_memory = None\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.start_method = fork\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.cuda_device = None\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.quiet = False\n",
            "2022-04-15 16:11:31,532 - INFO - allennlp.common.params - validation_data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f683d5d7d60>\n",
            "loading instances: 0it [00:00, ?it/s]2022-04-15 16:11:31,533 - INFO - seq2rel.dataset_reader - Reading instances from lines in file at: gda/valid.tsv\n",
            "loading instances: 16it [00:00, 213.83it/s]\n",
            "2022-04-15 16:11:31,608 - INFO - allennlp.common.params - vocabulary.type = from_instances\n",
            "2022-04-15 16:11:31,608 - INFO - allennlp.common.params - vocabulary.min_count = None\n",
            "2022-04-15 16:11:31,608 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
            "2022-04-15 16:11:31,608 - INFO - allennlp.common.params - vocabulary.pretrained_files = None\n",
            "2022-04-15 16:11:31,608 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n",
            "2022-04-15 16:11:31,609 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n",
            "2022-04-15 16:11:31,609 - INFO - allennlp.common.params - vocabulary.padding_token = @@PADDING@@\n",
            "2022-04-15 16:11:31,609 - INFO - allennlp.common.params - vocabulary.oov_token = @@UNKNOWN@@\n",
            "2022-04-15 16:11:31,609 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 32it [00:00, 22206.77it/s]\n",
            "2022-04-15 16:11:31,611 - INFO - allennlp.common.params - model.type = copynet_seq2rel\n",
            "2022-04-15 16:11:31,611 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.type = basic\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.type = pretrained_transformer\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.max_length = None\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.sub_module = None\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.train_parameters = True\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.eval_mode = False\n",
            "2022-04-15 16:11:31,612 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2022-04-15 16:11:31,613 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2022-04-15 16:11:31,613 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2022-04-15 16:11:31,613 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.reinit_modules = 1\n",
            "2022-04-15 16:11:31,613 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.load_weights = True\n",
            "2022-04-15 16:11:31,613 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2022-04-15 16:11:31,613 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.tokenizer_kwargs.do_lower_case = True\n",
            "2022-04-15 16:11:31,613 - INFO - allennlp.common.params - model.source_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "Downloading: 100% 420M/420M [00:05<00:00, 81.0MB/s]\n",
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2022-04-15 16:11:40,633 - INFO - allennlp.common.params - model.encoder = None\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.label_smoothing = None\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.target_embedding_dim = 256\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.scheduled_sampling_ratio = 0.0\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.copy_token = @COPY@\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.target_namespace = target_tokens\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.tensor_based_metric = None\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.token_based_metric = None\n",
            "2022-04-15 16:11:40,634 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f68371cc6d0>\n",
            "2022-04-15 16:11:40,635 - INFO - allennlp.common.params - model.target_tokenizer.type = pretrained_transformer\n",
            "2022-04-15 16:11:40,635 - INFO - allennlp.common.params - model.target_tokenizer.model_name = microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
            "2022-04-15 16:11:40,635 - INFO - allennlp.common.params - model.target_tokenizer.add_special_tokens = False\n",
            "2022-04-15 16:11:40,635 - INFO - allennlp.common.params - model.target_tokenizer.max_length = None\n",
            "2022-04-15 16:11:40,635 - INFO - allennlp.common.params - model.target_tokenizer.tokenizer_kwargs.additional_special_tokens = ['@GENE@', '@DISEASE@', '@GDA@', ';', '@start@', '@end@']\n",
            "2022-04-15 16:11:40,635 - INFO - allennlp.common.params - model.target_tokenizer.tokenizer_kwargs.do_lower_case = True\n",
            "2022-04-15 16:11:40,635 - INFO - allennlp.common.params - model.target_tokenizer.verification_tokens = None\n",
            "2022-04-15 16:11:40,636 - INFO - allennlp.common.params - model.dropout = 0.1\n",
            "2022-04-15 16:11:40,636 - INFO - allennlp.common.params - model.weight_dropout = 0.5\n",
            "2022-04-15 16:11:40,637 - INFO - allennlp.common.params - model.sequence_based_metrics.0.type = f1_seq2rel\n",
            "2022-04-15 16:11:40,637 - INFO - allennlp.common.params - model.sequence_based_metrics.0.labels = ['GDA']\n",
            "2022-04-15 16:11:40,637 - INFO - allennlp.common.params - model.sequence_based_metrics.0.threshold = None\n",
            "2022-04-15 16:11:40,637 - INFO - allennlp.common.params - model.sequence_based_metrics.0.ordered_ents = False\n",
            "2022-04-15 16:11:40,637 - INFO - allennlp.common.params - model.sequence_based_metrics.0.remove_duplicate_ents = True\n",
            "2022-04-15 16:11:40,637 - INFO - allennlp.common.params - model.sequence_based_metrics.0.average = micro\n",
            "2022-04-15 16:11:40,637 - INFO - allennlp.common.params - model.init_decoder_state_strategy = mean\n",
            "2022-04-15 16:11:40,638 - INFO - allennlp.common.params - model.attention.type = multihead_attention\n",
            "2022-04-15 16:11:40,638 - INFO - allennlp.common.params - model.attention.normalize = True\n",
            "2022-04-15 16:11:40,638 - INFO - allennlp.common.params - model.attention.num_heads = 6\n",
            "2022-04-15 16:11:40,705 - INFO - allennlp.common.params - model.beam_search.type = beam_search\n",
            "2022-04-15 16:11:40,705 - INFO - allennlp.common.params - model.beam_search.max_steps = 96\n",
            "2022-04-15 16:11:40,705 - INFO - allennlp.common.params - model.beam_search.beam_size = 4\n",
            "2022-04-15 16:11:40,705 - INFO - allennlp.common.params - model.beam_search.per_node_beam_size = None\n",
            "2022-04-15 16:11:40,705 - INFO - allennlp.common.params - model.beam_search.sampler = None\n",
            "2022-04-15 16:11:40,705 - INFO - allennlp.common.params - model.beam_search.min_steps = None\n",
            "2022-04-15 16:11:40,706 - INFO - allennlp.common.params - model.beam_search.final_sequence_scorer.type = length-normalized-sequence-log-prob\n",
            "2022-04-15 16:11:40,706 - INFO - allennlp.common.params - model.beam_search.final_sequence_scorer.length_penalty = 0.8\n",
            "2022-04-15 16:11:40,706 - INFO - allennlp.common.params - model.beam_search.constraints = None\n",
            "2022-04-15 16:11:40,706 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2022-04-15 16:11:40,707 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2022-04-15 16:11:40,707 - INFO - allennlp.nn.initializers -    _attention._multihead_attn.in_proj_bias\n",
            "2022-04-15 16:11:40,707 - INFO - allennlp.nn.initializers -    _attention._multihead_attn.in_proj_weight\n",
            "2022-04-15 16:11:40,707 - INFO - allennlp.nn.initializers -    _attention._multihead_attn.out_proj.bias\n",
            "2022-04-15 16:11:40,707 - INFO - allennlp.nn.initializers -    _attention._multihead_attn.out_proj.weight\n",
            "2022-04-15 16:11:40,707 - INFO - allennlp.nn.initializers -    _decoder_cell.bias_hh\n",
            "2022-04-15 16:11:40,707 - INFO - allennlp.nn.initializers -    _decoder_cell.bias_ih\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _decoder_cell.weight_hh\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _decoder_cell.weight_ih\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _input_projection_layer.bias\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _input_projection_layer.weight\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _output_copying_layer.bias\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _output_copying_layer.weight\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _output_generation_layer.bias\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _output_generation_layer.weight\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight\n",
            "2022-04-15 16:11:40,708 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2022-04-15 16:11:40,709 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2022-04-15 16:11:40,710 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,711 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,712 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,713 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2022-04-15 16:11:40,714 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,715 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,715 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,715 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,715 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2022-04-15 16:11:40,715 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2022-04-15 16:11:40,715 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,715 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,798 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,798 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,798 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2022-04-15 16:11:40,798 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2022-04-15 16:11:40,798 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2022-04-15 16:11:40,799 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2022-04-15 16:11:40,800 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,800 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,800 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,800 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,800 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2022-04-15 16:11:40,800 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2022-04-15 16:11:40,801 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2022-04-15 16:11:40,802 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2022-04-15 16:11:40,803 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2022-04-15 16:11:40,804 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2022-04-15 16:11:40,805 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2022-04-15 16:11:40,806 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias\n",
            "2022-04-15 16:11:40,806 - INFO - allennlp.nn.initializers -    _source_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight\n",
            "2022-04-15 16:11:40,806 - INFO - allennlp.nn.initializers -    _target_embedder.weight\n",
            "2022-04-15 16:11:40,838 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2022-04-15 16:11:40,838 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.distributed = False\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.patience = None\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.validation_metric = +fscore\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.num_epochs = 30\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.grad_norm = 1\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.use_amp = True\n",
            "2022-04-15 16:11:40,839 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2022-04-15 16:11:40,840 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2022-04-15 16:11:40,840 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2022-04-15 16:11:40,840 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f68373ea280>\n",
            "2022-04-15 16:11:40,840 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n",
            "2022-04-15 16:11:40,840 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n",
            "2022-04-15 16:11:40,840 - INFO - allennlp.common.params - trainer.grad_scaling = True\n",
            "2022-04-15 16:11:44,300 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2022-04-15 16:11:44,300 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.0005\n",
            "2022-04-15 16:11:44,300 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2022-04-15 16:11:44,301 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
            "2022-04-15 16:11:44,301 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0\n",
            "2022-04-15 16:11:44,301 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True\n",
            "2022-04-15 16:11:44,302 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n",
            "2022-04-15 16:11:44,302 - INFO - allennlp.training.optimizers - Group 0: ['_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight'], {'lr': 2e-05, 'weight_decay': 0.01}\n",
            "2022-04-15 16:11:44,302 - INFO - allennlp.training.optimizers - Group 1: ['_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias', '_source_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias', '_source_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias', '_source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias'], {'lr': 2e-05, 'weight_decay': 0}\n",
            "2022-04-15 16:11:44,336 - INFO - allennlp.training.optimizers - Group 2: ['_output_copying_layer.weight', '_attention._multihead_attn.in_proj_bias', '_decoder_cell.module.bias_ih', '_output_generation_layer.bias', '_attention._multihead_attn.in_proj_weight', '_target_embedder.weight', '_decoder_cell.module.bias_hh', '_output_generation_layer.weight', '_input_projection_layer.bias', '_decoder_cell.weight_hh_raw', '_input_projection_layer.weight', '_attention._multihead_attn.out_proj.weight', '_attention._multihead_attn.out_proj.bias', '_decoder_cell.module.weight_ih', '_output_copying_layer.bias'], {}\n",
            "2022-04-15 16:11:44,338 - INFO - allennlp.training.optimizers - Number of trainable parameters: 118546185\n",
            "/usr/local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "2022-04-15 16:11:44,345 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2022-04-15 16:11:44,346 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2022-04-15 16:11:44,347 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2022-04-15 16:11:44,348 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2022-04-15 16:11:44,349 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,350 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2022-04-15 16:11:44,351 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2022-04-15 16:11:44,352 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,353 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2022-04-15 16:11:44,354 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2022-04-15 16:11:44,354 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,354 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,354 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2022-04-15 16:11:44,442 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,443 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2022-04-15 16:11:44,444 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2022-04-15 16:11:44,445 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2022-04-15 16:11:44,446 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,447 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2022-04-15 16:11:44,448 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _source_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _target_embedder.weight\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _attention._multihead_attn.in_proj_weight\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _attention._multihead_attn.in_proj_bias\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _attention._multihead_attn.out_proj.weight\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _attention._multihead_attn.out_proj.bias\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _input_projection_layer.weight\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _input_projection_layer.bias\n",
            "2022-04-15 16:11:44,449 - INFO - allennlp.common.util - _decoder_cell.weight_hh_raw\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.util - _decoder_cell.module.weight_ih\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.util - _decoder_cell.module.bias_ih\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.util - _decoder_cell.module.bias_hh\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.util - _output_generation_layer.weight\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.util - _output_generation_layer.bias\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.util - _output_copying_layer.weight\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.util - _output_copying_layer.bias\n",
            "2022-04-15 16:11:44,450 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = linear_with_warmup\n",
            "2022-04-15 16:11:44,451 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.warmup_steps = 21675\n",
            "2022-04-15 16:11:44,451 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2022-04-15 16:11:44,451 - INFO - allennlp.common.params - type = default\n",
            "2022-04-15 16:11:44,451 - INFO - allennlp.common.params - save_completed_epochs = True\n",
            "2022-04-15 16:11:44,452 - INFO - allennlp.common.params - save_every_num_seconds = None\n",
            "2022-04-15 16:11:44,452 - INFO - allennlp.common.params - save_every_num_batches = None\n",
            "2022-04-15 16:11:44,452 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n",
            "2022-04-15 16:11:44,452 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n",
            "2022-04-15 16:11:44,452 - INFO - allennlp.common.params - trainer.callbacks.0.type = should_validate_callback\n",
            "2022-04-15 16:11:44,452 - INFO - allennlp.common.params - trainer.callbacks.0.validation_start = 29\n",
            "2022-04-15 16:11:44,452 - INFO - allennlp.common.params - trainer.callbacks.0.validation_interval = 1\n",
            "2022-04-15 16:11:44,453 - WARNING - allennlp.training.gradient_descent_trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "2022-04-15 16:11:44,455 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n",
            "2022-04-15 16:11:44,455 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/29\n",
            "2022-04-15 16:11:44,455 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:11:44,456 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 453M\n",
            "2022-04-15 16:11:44,456 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]2022-04-15 16:11:44,653 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n",
            "2022-04-15 16:11:44,653 - INFO - allennlp.training.callbacks.console_logger - batch_input/source_tokens/tokens/token_ids (Shape: 4 x 260)\n",
            "tensor([[    2,  4824,  1927,  ...,     0,     0,     0],\n",
            "        [    2,  5619,    17,  ...,     0,     0,     0],\n",
            "        [    2,  8321,  1927,  ...,     0,     0,     0],\n",
            "        [    2,  2991, 18675,  ...,  2740,    18,     3]], device='cuda:0')\n",
            "2022-04-15 16:11:44,655 - INFO - allennlp.training.callbacks.console_logger - batch_input/source_tokens/tokens/mask (Shape: 4 x 260)\n",
            "tensor([[ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')\n",
            "2022-04-15 16:11:44,656 - INFO - allennlp.training.callbacks.console_logger - batch_input/source_tokens/tokens/type_ids (Shape: 4 x 260)\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
            "2022-04-15 16:11:44,658 - INFO - allennlp.training.callbacks.console_logger - batch_input/source_to_target (Shape: 4 x 260)\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')\n",
            "2022-04-15 16:11:44,660 - INFO - allennlp.training.callbacks.console_logger - batch_input/target_tokens/tokens/tokens (Shape: 4 x 14)\n",
            "tensor([[6, 1, 3,  ..., 0, 0, 0],\n",
            "        [6, 1, 3,  ..., 5, 7, 0],\n",
            "        [6, 1, 1,  ..., 4, 5, 7],\n",
            "        [6, 1, 1,  ..., 4, 5, 7]], device='cuda:0')\n",
            "2022-04-15 16:11:44,663 - INFO - allennlp.training.callbacks.console_logger - batch_input/source_token_ids (Shape: 4 x 260)\n",
            "tensor([[  0,   1,   2,  ...,   0,   0,   0],\n",
            "        [  0,   1,   2,  ...,   0,   0,   0],\n",
            "        [  0,   1,   2,  ...,   0,   0,   0],\n",
            "        [  0,   1,   2,  ...,   6,  19, 118]], device='cuda:0')\n",
            "2022-04-15 16:11:44,665 - INFO - allennlp.training.callbacks.console_logger - batch_input/target_token_ids (Shape: 4 x 14)\n",
            "tensor([[ 27,  17,  28,  ...,   0,   0,   0],\n",
            "        [ 29,  21,  30,  ...,  32,  33,   0],\n",
            "        [ 79,   4,   5,  ...,  82,  83,  84],\n",
            "        [119,   1,   2,  ..., 122, 123, 124]], device='cuda:0')\n",
            "2022-04-15 16:11:44,666 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/metadata\" : (Length 4 of type \"<class 'dict'>\")\n",
            "batch_loss: 90.6991, loss: 85.6036 ||: 100%|##########| 4/4 [00:00<00:00,  4.28it/s]\n",
            "2022-04-15 16:11:45,394 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:11:45,394 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |   453.306  |       N/A\n",
            "2022-04-15 16:11:45,395 - INFO - allennlp.training.callbacks.console_logger - loss               |    85.604  |       N/A\n",
            "2022-04-15 16:11:45,395 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3908.750  |       N/A\n",
            "2022-04-15 16:11:46,794 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:02.339568\n",
            "2022-04-15 16:11:46,795 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:27\n",
            "2022-04-15 16:11:46,795 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/29\n",
            "2022-04-15 16:11:46,795 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:11:46,795 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 3.5G\n",
            "2022-04-15 16:11:46,797 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 116.4123, loss: 84.9654 ||: 100%|##########| 4/4 [00:01<00:00,  3.29it/s]\n",
            "2022-04-15 16:11:48,016 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:11:48,016 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  3564.663  |       N/A\n",
            "2022-04-15 16:11:48,016 - INFO - allennlp.training.callbacks.console_logger - loss               |    84.965  |       N/A\n",
            "2022-04-15 16:11:48,017 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3918.324  |       N/A\n",
            "2022-04-15 16:11:52,837 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:06.041913\n",
            "2022-04-15 16:11:52,837 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:49\n",
            "2022-04-15 16:11:52,837 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/29\n",
            "2022-04-15 16:11:52,838 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:11:52,838 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:11:52,840 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 108.3218, loss: 85.6357 ||: 100%|##########| 4/4 [00:00<00:00,  4.11it/s]\n",
            "2022-04-15 16:11:53,820 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:11:53,820 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4466.373  |       N/A\n",
            "2022-04-15 16:11:53,820 - INFO - allennlp.training.callbacks.console_logger - loss               |    85.636  |       N/A\n",
            "2022-04-15 16:11:53,820 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:11:58,679 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.841330\n",
            "2022-04-15 16:11:58,679 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:24\n",
            "2022-04-15 16:11:58,679 - INFO - allennlp.training.gradient_descent_trainer - Epoch 3/29\n",
            "2022-04-15 16:11:58,679 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:11:58,680 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:11:58,681 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 56.3955, loss: 84.8171 ||: 100%|##########| 4/4 [00:01<00:00,  3.90it/s]\n",
            "2022-04-15 16:11:59,710 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:11:59,710 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4470.373  |       N/A\n",
            "2022-04-15 16:11:59,710 - INFO - allennlp.training.callbacks.console_logger - loss               |    84.817  |       N/A\n",
            "2022-04-15 16:11:59,710 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:04,716 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:06.036478\n",
            "2022-04-15 16:12:04,716 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:39\n",
            "2022-04-15 16:12:04,716 - INFO - allennlp.training.gradient_descent_trainer - Epoch 4/29\n",
            "2022-04-15 16:12:04,716 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:04,717 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:04,719 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 115.0749, loss: 84.3683 ||: 100%|##########| 4/4 [00:01<00:00,  3.96it/s]\n",
            "2022-04-15 16:12:05,733 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:05,733 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4474.498  |       N/A\n",
            "2022-04-15 16:12:05,733 - INFO - allennlp.training.callbacks.console_logger - loss               |    84.368  |       N/A\n",
            "2022-04-15 16:12:05,733 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:10,730 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:06.014016\n",
            "2022-04-15 16:12:10,730 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:46\n",
            "2022-04-15 16:12:10,731 - INFO - allennlp.training.gradient_descent_trainer - Epoch 5/29\n",
            "2022-04-15 16:12:10,731 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:10,731 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:10,732 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 84.2137, loss: 83.6736 ||: 100%|##########| 4/4 [00:01<00:00,  3.98it/s]\n",
            "2022-04-15 16:12:11,739 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:11,740 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4470.373  |       N/A\n",
            "2022-04-15 16:12:11,740 - INFO - allennlp.training.callbacks.console_logger - loss               |    83.674  |       N/A\n",
            "2022-04-15 16:12:11,740 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:16,667 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.936394\n",
            "2022-04-15 16:12:16,667 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:49\n",
            "2022-04-15 16:12:16,667 - INFO - allennlp.training.gradient_descent_trainer - Epoch 6/29\n",
            "2022-04-15 16:12:16,668 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:16,668 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:16,670 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 103.5196, loss: 83.5529 ||: 100%|##########| 4/4 [00:00<00:00,  4.06it/s]\n",
            "2022-04-15 16:12:17,657 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:17,658 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4474.498  |       N/A\n",
            "2022-04-15 16:12:17,658 - INFO - allennlp.training.callbacks.console_logger - loss               |    83.553  |       N/A\n",
            "2022-04-15 16:12:17,658 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:22,551 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.883511\n",
            "2022-04-15 16:12:22,589 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:49\n",
            "2022-04-15 16:12:22,589 - INFO - allennlp.training.gradient_descent_trainer - Epoch 7/29\n",
            "2022-04-15 16:12:22,589 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:22,590 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:22,591 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 83.8321, loss: 82.5435 ||: 100%|##########| 4/4 [00:01<00:00,  3.83it/s]\n",
            "2022-04-15 16:12:23,638 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:23,638 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4470.373  |       N/A\n",
            "2022-04-15 16:12:23,638 - INFO - allennlp.training.callbacks.console_logger - loss               |    82.543  |       N/A\n",
            "2022-04-15 16:12:23,639 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:28,554 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.964367\n",
            "2022-04-15 16:12:28,554 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:47\n",
            "2022-04-15 16:12:28,554 - INFO - allennlp.training.gradient_descent_trainer - Epoch 8/29\n",
            "2022-04-15 16:12:28,554 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:28,555 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:28,557 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 53.0132, loss: 81.2115 ||: 100%|##########| 4/4 [00:00<00:00,  4.14it/s]\n",
            "2022-04-15 16:12:29,525 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:29,525 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4474.498  |       N/A\n",
            "2022-04-15 16:12:29,526 - INFO - allennlp.training.callbacks.console_logger - loss               |    81.211  |       N/A\n",
            "2022-04-15 16:12:29,526 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:34,445 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.890359\n",
            "2022-04-15 16:12:34,445 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:45\n",
            "2022-04-15 16:12:34,445 - INFO - allennlp.training.gradient_descent_trainer - Epoch 9/29\n",
            "2022-04-15 16:12:34,445 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:34,445 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:34,446 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 108.9817, loss: 80.3763 ||: 100%|##########| 4/4 [00:01<00:00,  3.97it/s]\n",
            "2022-04-15 16:12:35,456 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:35,456 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4474.498  |       N/A\n",
            "2022-04-15 16:12:35,456 - INFO - allennlp.training.callbacks.console_logger - loss               |    80.376  |       N/A\n",
            "2022-04-15 16:12:35,456 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:40,351 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.906289\n",
            "2022-04-15 16:12:40,408 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:42\n",
            "2022-04-15 16:12:40,408 - INFO - allennlp.training.gradient_descent_trainer - Epoch 10/29\n",
            "2022-04-15 16:12:40,408 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:40,409 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:40,410 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 91.8275, loss: 80.0972 ||: 100%|##########| 4/4 [00:01<00:00,  3.94it/s]\n",
            "2022-04-15 16:12:41,431 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:41,431 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4470.373  |       N/A\n",
            "2022-04-15 16:12:41,431 - INFO - allennlp.training.callbacks.console_logger - loss               |    80.097  |       N/A\n",
            "2022-04-15 16:12:41,431 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:46,307 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.898656\n",
            "2022-04-15 16:12:46,307 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:38\n",
            "2022-04-15 16:12:46,307 - INFO - allennlp.training.gradient_descent_trainer - Epoch 11/29\n",
            "2022-04-15 16:12:46,307 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:46,308 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:46,310 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 73.2910, loss: 78.1378 ||: 100%|##########| 4/4 [00:00<00:00,  4.13it/s]\n",
            "2022-04-15 16:12:47,283 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:47,283 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4471.873  |       N/A\n",
            "2022-04-15 16:12:47,283 - INFO - allennlp.training.callbacks.console_logger - loss               |    78.138  |       N/A\n",
            "2022-04-15 16:12:47,283 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:52,159 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.851501\n",
            "2022-04-15 16:12:52,159 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:34\n",
            "2022-04-15 16:12:52,159 - INFO - allennlp.training.gradient_descent_trainer - Epoch 12/29\n",
            "2022-04-15 16:12:52,159 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:52,160 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:52,162 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 98.4853, loss: 77.4337 ||: 100%|##########| 4/4 [00:01<00:00,  3.93it/s]\n",
            "2022-04-15 16:12:53,184 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:53,184 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4474.498  |       N/A\n",
            "2022-04-15 16:12:53,184 - INFO - allennlp.training.callbacks.console_logger - loss               |    77.434  |       N/A\n",
            "2022-04-15 16:12:53,184 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:12:58,041 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.882094\n",
            "2022-04-15 16:12:58,121 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:29\n",
            "2022-04-15 16:12:58,121 - INFO - allennlp.training.gradient_descent_trainer - Epoch 13/29\n",
            "2022-04-15 16:12:58,121 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:12:58,122 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:12:58,126 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 84.8134, loss: 76.7163 ||: 100%|##########| 4/4 [00:01<00:00,  3.98it/s]\n",
            "2022-04-15 16:12:59,136 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:12:59,136 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4473.373  |       N/A\n",
            "2022-04-15 16:12:59,136 - INFO - allennlp.training.callbacks.console_logger - loss               |    76.716  |       N/A\n",
            "2022-04-15 16:12:59,136 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:04,014 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.892826\n",
            "2022-04-15 16:13:04,015 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:25\n",
            "2022-04-15 16:13:04,015 - INFO - allennlp.training.gradient_descent_trainer - Epoch 14/29\n",
            "2022-04-15 16:13:04,015 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:04,015 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.7G\n",
            "2022-04-15 16:13:04,017 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 49.8543, loss: 75.8552 ||: 100%|##########| 4/4 [00:01<00:00,  3.89it/s]\n",
            "2022-04-15 16:13:05,046 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:05,046 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4824.151  |       N/A\n",
            "2022-04-15 16:13:05,047 - INFO - allennlp.training.callbacks.console_logger - loss               |    75.855  |       N/A\n",
            "2022-04-15 16:13:05,047 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:09,884 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.869704\n",
            "2022-04-15 16:13:09,885 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:20\n",
            "2022-04-15 16:13:09,885 - INFO - allennlp.training.gradient_descent_trainer - Epoch 15/29\n",
            "2022-04-15 16:13:09,885 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:09,885 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.7G\n",
            "2022-04-15 16:13:09,887 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 96.3272, loss: 73.7622 ||: 100%|##########| 4/4 [00:01<00:00,  3.20it/s]\n",
            "2022-04-15 16:13:11,137 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:11,138 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4824.151  |       N/A\n",
            "2022-04-15 16:13:11,138 - INFO - allennlp.training.callbacks.console_logger - loss               |    73.762  |       N/A\n",
            "2022-04-15 16:13:11,138 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:16,045 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:06.159893\n",
            "2022-04-15 16:13:16,045 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:15\n",
            "2022-04-15 16:13:16,045 - INFO - allennlp.training.gradient_descent_trainer - Epoch 16/29\n",
            "2022-04-15 16:13:16,045 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:16,046 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.7G\n",
            "2022-04-15 16:13:16,047 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 98.9083, loss: 72.3108 ||: 100%|##########| 4/4 [00:00<00:00,  4.02it/s]\n",
            "2022-04-15 16:13:17,044 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:17,044 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4824.151  |       N/A\n",
            "2022-04-15 16:13:17,045 - INFO - allennlp.training.callbacks.console_logger - loss               |    72.311  |       N/A\n",
            "2022-04-15 16:13:17,045 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:21,911 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.865832\n",
            "2022-04-15 16:13:21,911 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:10\n",
            "2022-04-15 16:13:21,911 - INFO - allennlp.training.gradient_descent_trainer - Epoch 17/29\n",
            "2022-04-15 16:13:21,912 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:21,912 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:13:21,913 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 56.8438, loss: 71.4328 ||: 100%|##########| 4/4 [00:00<00:00,  4.02it/s]\n",
            "2022-04-15 16:13:22,910 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:22,910 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4477.498  |       N/A\n",
            "2022-04-15 16:13:22,910 - INFO - allennlp.training.callbacks.console_logger - loss               |    71.433  |       N/A\n",
            "2022-04-15 16:13:22,910 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:27,838 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.926323\n",
            "2022-04-15 16:13:27,879 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:05\n",
            "2022-04-15 16:13:27,879 - INFO - allennlp.training.gradient_descent_trainer - Epoch 18/29\n",
            "2022-04-15 16:13:27,880 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:27,880 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:13:27,882 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 66.6216, loss: 70.3496 ||: 100%|##########| 4/4 [00:01<00:00,  3.95it/s]\n",
            "2022-04-15 16:13:28,898 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:28,898 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4479.373  |       N/A\n",
            "2022-04-15 16:13:28,898 - INFO - allennlp.training.callbacks.console_logger - loss               |    70.350  |       N/A\n",
            "2022-04-15 16:13:28,898 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:33,802 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.922391\n",
            "2022-04-15 16:13:33,802 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:01:00\n",
            "2022-04-15 16:13:33,802 - INFO - allennlp.training.gradient_descent_trainer - Epoch 19/29\n",
            "2022-04-15 16:13:33,802 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:33,803 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:13:33,804 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 68.3864, loss: 67.9640 ||: 100%|##########| 4/4 [00:00<00:00,  4.01it/s]\n",
            "2022-04-15 16:13:34,803 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:34,803 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4480.615  |       N/A\n",
            "2022-04-15 16:13:34,803 - INFO - allennlp.training.callbacks.console_logger - loss               |    67.964  |       N/A\n",
            "2022-04-15 16:13:34,803 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:39,697 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.894507\n",
            "2022-04-15 16:13:39,697 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:55\n",
            "2022-04-15 16:13:39,697 - INFO - allennlp.training.gradient_descent_trainer - Epoch 20/29\n",
            "2022-04-15 16:13:39,698 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:39,698 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:13:39,699 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 85.0915, loss: 67.3344 ||: 100%|##########| 4/4 [00:01<00:00,  3.93it/s]\n",
            "2022-04-15 16:13:40,719 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:40,719 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4476.373  |       N/A\n",
            "2022-04-15 16:13:40,720 - INFO - allennlp.training.callbacks.console_logger - loss               |    67.334  |       N/A\n",
            "2022-04-15 16:13:40,720 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:45,554 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.856756\n",
            "2022-04-15 16:13:45,605 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:49\n",
            "2022-04-15 16:13:45,605 - INFO - allennlp.training.gradient_descent_trainer - Epoch 21/29\n",
            "2022-04-15 16:13:45,605 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:45,606 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:13:45,607 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 94.0378, loss: 65.3923 ||: 100%|##########| 4/4 [00:01<00:00,  3.93it/s]\n",
            "2022-04-15 16:13:46,628 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:46,628 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4477.873  |       N/A\n",
            "2022-04-15 16:13:46,628 - INFO - allennlp.training.callbacks.console_logger - loss               |    65.392  |       N/A\n",
            "2022-04-15 16:13:46,628 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:51,441 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.836383\n",
            "2022-04-15 16:13:51,441 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:44\n",
            "2022-04-15 16:13:51,442 - INFO - allennlp.training.gradient_descent_trainer - Epoch 22/29\n",
            "2022-04-15 16:13:51,442 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:51,442 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:13:51,443 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 63.2003, loss: 64.1470 ||: 100%|##########| 4/4 [00:01<00:00,  3.95it/s]\n",
            "2022-04-15 16:13:52,460 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:52,460 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4477.498  |       N/A\n",
            "2022-04-15 16:13:52,460 - INFO - allennlp.training.callbacks.console_logger - loss               |    64.147  |       N/A\n",
            "2022-04-15 16:13:52,460 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:13:57,340 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.898686\n",
            "2022-04-15 16:13:57,341 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:38\n",
            "2022-04-15 16:13:57,341 - INFO - allennlp.training.gradient_descent_trainer - Epoch 23/29\n",
            "2022-04-15 16:13:57,341 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:13:57,341 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:13:57,342 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 53.4328, loss: 63.2852 ||: 100%|##########| 4/4 [00:01<00:00,  3.77it/s]\n",
            "2022-04-15 16:13:58,406 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:13:58,407 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4480.615  |       N/A\n",
            "2022-04-15 16:13:58,407 - INFO - allennlp.training.callbacks.console_logger - loss               |    63.285  |       N/A\n",
            "2022-04-15 16:13:58,407 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:14:03,277 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.936144\n",
            "2022-04-15 16:14:03,310 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:33\n",
            "2022-04-15 16:14:03,311 - INFO - allennlp.training.gradient_descent_trainer - Epoch 24/29\n",
            "2022-04-15 16:14:03,311 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:14:03,311 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:14:03,313 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 51.5458, loss: 61.1220 ||: 100%|##########| 4/4 [00:01<00:00,  3.93it/s]\n",
            "2022-04-15 16:14:04,333 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:14:04,333 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4477.498  |       N/A\n",
            "2022-04-15 16:14:04,333 - INFO - allennlp.training.callbacks.console_logger - loss               |    61.122  |       N/A\n",
            "2022-04-15 16:14:04,333 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:14:09,189 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.878776\n",
            "2022-04-15 16:14:09,190 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:27\n",
            "2022-04-15 16:14:09,190 - INFO - allennlp.training.gradient_descent_trainer - Epoch 25/29\n",
            "2022-04-15 16:14:09,190 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:14:09,192 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:14:09,193 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 69.3625, loss: 60.0567 ||: 100%|##########| 4/4 [00:01<00:00,  3.83it/s]\n",
            "2022-04-15 16:14:10,239 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:14:10,239 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4480.615  |       N/A\n",
            "2022-04-15 16:14:10,240 - INFO - allennlp.training.callbacks.console_logger - loss               |    60.057  |       N/A\n",
            "2022-04-15 16:14:10,240 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:14:15,091 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.900701\n",
            "2022-04-15 16:14:15,091 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:22\n",
            "2022-04-15 16:14:15,091 - INFO - allennlp.training.gradient_descent_trainer - Epoch 26/29\n",
            "2022-04-15 16:14:15,091 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:14:15,092 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:14:15,094 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 39.9142, loss: 60.0555 ||: 100%|##########| 4/4 [00:00<00:00,  4.09it/s]\n",
            "2022-04-15 16:14:16,076 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:14:16,076 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4479.373  |       N/A\n",
            "2022-04-15 16:14:16,076 - INFO - allennlp.training.callbacks.console_logger - loss               |    60.055  |       N/A\n",
            "2022-04-15 16:14:16,076 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:14:20,942 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.850787\n",
            "2022-04-15 16:14:21,000 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:16\n",
            "2022-04-15 16:14:21,000 - INFO - allennlp.training.gradient_descent_trainer - Epoch 27/29\n",
            "2022-04-15 16:14:21,000 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:14:21,001 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:14:21,002 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 53.2325, loss: 59.7149 ||: 100%|##########| 4/4 [00:01<00:00,  3.92it/s]\n",
            "2022-04-15 16:14:22,024 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:14:22,025 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4476.373  |       N/A\n",
            "2022-04-15 16:14:22,025 - INFO - allennlp.training.callbacks.console_logger - loss               |    59.715  |       N/A\n",
            "2022-04-15 16:14:22,025 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:14:26,935 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.934801\n",
            "2022-04-15 16:14:26,935 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:11\n",
            "2022-04-15 16:14:26,936 - INFO - allennlp.training.gradient_descent_trainer - Epoch 28/29\n",
            "2022-04-15 16:14:26,936 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:14:26,936 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:14:26,938 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 55.7468, loss: 57.9086 ||: 100%|##########| 4/4 [00:01<00:00,  3.97it/s]\n",
            "2022-04-15 16:14:27,948 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:14:27,948 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4480.615  |       N/A\n",
            "2022-04-15 16:14:27,948 - INFO - allennlp.training.callbacks.console_logger - loss               |    57.909  |       N/A\n",
            "2022-04-15 16:14:27,948 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:14:32,835 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:05.898958\n",
            "2022-04-15 16:14:32,835 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:00:05\n",
            "2022-04-15 16:14:32,835 - INFO - allennlp.training.gradient_descent_trainer - Epoch 29/29\n",
            "2022-04-15 16:14:32,835 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 3.8G\n",
            "/usr/local/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-04-15 16:14:32,836 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 4.4G\n",
            "2022-04-15 16:14:32,838 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "batch_loss: 52.1818, loss: 57.4284 ||: 100%|##########| 4/4 [00:01<00:00,  3.89it/s]\n",
            "2022-04-15 16:14:33,868 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "precision: 0.0000, recall: 0.0000, fscore: 0.0000, batch_loss: 73.5058, loss: 73.5058 ||: 100%|##########| 1/1 [00:18<00:00, 18.96s/it]\n",
            "2022-04-15 16:14:52,833 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2022-04-15 16:14:52,833 - INFO - allennlp.training.callbacks.console_logger - fscore             |       N/A  |     0.000\n",
            "2022-04-15 16:14:52,833 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  4479.373  |       N/A\n",
            "2022-04-15 16:14:52,833 - INFO - allennlp.training.callbacks.console_logger - loss               |    57.428  |    73.506\n",
            "2022-04-15 16:14:52,833 - INFO - allennlp.training.callbacks.console_logger - precision          |       N/A  |     0.000\n",
            "2022-04-15 16:14:52,833 - INFO - allennlp.training.callbacks.console_logger - recall             |       N/A  |     0.000\n",
            "2022-04-15 16:14:52,833 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  3919.055  |       N/A\n",
            "2022-04-15 16:14:57,659 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:00:24.824323\n",
            "2022-04-15 16:14:57,660 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 29,\n",
            "  \"peak_worker_0_memory_MB\": 3919.0546875,\n",
            "  \"peak_gpu_0_memory_MB\": 4824.1513671875,\n",
            "  \"training_duration\": \"0:03:08.377401\",\n",
            "  \"epoch\": 29,\n",
            "  \"training_loss\": 57.428375244140625,\n",
            "  \"training_worker_0_memory_MB\": 3919.0546875,\n",
            "  \"training_gpu_0_memory_MB\": 4479.37255859375,\n",
            "  \"validation_precision\": 0.0,\n",
            "  \"validation_recall\": 0.0,\n",
            "  \"validation_fscore\": 0.0,\n",
            "  \"validation_loss\": 73.50579071044922,\n",
            "  \"best_validation_precision\": 0.0,\n",
            "  \"best_validation_recall\": 0.0,\n",
            "  \"best_validation_fscore\": 0.0,\n",
            "  \"best_validation_loss\": 73.50579071044922\n",
            "}\n",
            "2022-04-15 16:14:57,660 - INFO - allennlp.models.archival - archiving weights and vocabulary to output/model.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model checkpoint (measured by micro-F1 score on the validation set), vocabulary, configuration, and log files will be saved to `--serialization-dir`. This can be changed to any directory you like."
      ],
      "metadata": {
        "id": "iWIQWYBeBBXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## â™»ï¸ Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to collect data for training the model. We then briefly covered configuring and running a training session. Please see [our paper](https://aclanthology.org/2022.bionlp-1.2/) and [repo](https://github.com/JohnGiorgi/seq2rel) for more details, and don't hesitate to open an issue if you have any trouble!\n",
        "\n"
      ],
      "metadata": {
        "id": "7ivnfZjXb7BK"
      }
    }
  ]
}